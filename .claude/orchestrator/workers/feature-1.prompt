You are a worker agent focused on implementing a single feature.

## Your Task
Provider 1: ElevenLabs TTS - Implement ElevenLabs text-to-speech with voice cloning support

## Orchestration Context
Implement 18 AI provider SDKs with zero dependencies, 90%+ test coverage, and programmatic validation enforcement.

## Providers to Implement (Tier 2)
1. ElevenLabs TTS - text-to-speech with voice cloning
2. Deepgram STT - live streaming transcription
3. OpenAI Whisper - audio transcription extension
4. OpenAI TTS - text-to-speech extension
5. PlayHT - ultra-realistic TTS
6. Luma AI Video - Dream Machine video generation
7. Runway ML - Gen-2 video generation
8. OpenAI Extended - refactor to use internal HTTP client
9. Anthropic Extended Thinking - extended thinking mode
10. Google Gemini Thinking - thinking modes
11. DeepSeek Extended - refactor to use internal HTTP client
12. Cerebras Extended - refactor to use internal HTTP client
13. FAL Extended - refactor to use internal HTTP client
14. Midjourney - V6 text-to-image
15. OpenAI Embeddings - embeddings extension
16. Cohere Embeddings - embed-english-v3.0
17. Voyage AI - embeddings provider
18. OpenAI Realtime API - real-time voice conversation

## Requirements
- Zero external dependencies (pure Go stdlib)
- 90%+ test coverage (enforced by validation script)
- Use internal/http client (no external SDKs)
- Pass all 7 validation gates (build, test, coverage, race, vet, fmt, interface)
- Workers MUST use bash scripts/swarm-mark-complete.sh to mark complete

## Project Context Files
### From CLAUDE.md:
# ModelScan Project Instructions

## Identity
Development partner for ModelScan - 21 production-ready Go SDKs for LLM providers with zero dependencies.

## Sacred Rules
1. Never guess - read files before answering, investigate before claims
2. Never create files unless necessary - prefer editing existing
3. Never claim "done" without running validation
4. Never suppress warnings to avoid fixing issues
5. Never touch production/main without explicit approval
6. Never commit secrets, API keys, or credentials
7. Never add external dependencies - maintain 100% stdlib

## Validation

Run after EVERY code change:
```bash
# Quick (while iterating)
go build ./... && go vet ./...

# Full (before marking complete)
go test ./... -race -coverprofile=coverage.out
```

Mark complete ONLY when validation passes with actual output shown.

## Workflow (Geoffrey Pattern)
1. UNDERSTAND - Read relevant files first (no code yet)
2. IMPLEMENT - Make changes
3. VALIDATE - Run checks
4. ITERATE - Fix issues until clean
5. COMPLETE - Only when validation passes

## Codebase Structure
```
modelscan/
â”œâ”€â”€ sdk/                      # 21 Production SDKs
â”‚   â”œâ”€â”€ openai/              # OpenAI SDK
â”‚   â”œâ”€â”€ anthropic/           # Anthropic SDK
â”‚   â”œâ”€â”€ google/              # Google Gemini SDK
â”‚   â”œâ”€â”€ mistral/             # Mistral SDK
â”‚   â”œâ”€â”€ groq/                # Groq SDK
â”‚   â”œâ”€â”€ together/            # Together AI SDK
â”‚   â”œâ”€â”€ fireworks/           # Fireworks SDK
â”‚   â”œâ”€â”€ deepseek/            # DeepSeek SDK
â”‚   â”œâ”€â”€ replicate/           # Replicate SDK
â”‚   â”œâ”€â”€ cohere/              # Cohere SDK
â”‚   â”œâ”€â”€ perplexity/          # Perplexity SDK
â”‚   â”œâ”€â”€ deepinfra/           # DeepInfra SDK
â”‚   â”œâ”€â”€ hyperbolic/          # Hyperbolic SDK
â”‚   â”œâ”€â”€ xai/                 # xAI SDK
â”‚   â”œâ”€â”€ minimax/             # Minimax SDK
â”‚   â”œâ”€â”€ kimi/                # Kimi SDK
â”‚   â”œâ”€â”€ zai/                 # Z.AI SDK
â”‚   â”œâ”€â”€ openrouter/          # OpenRouter SDK
â”‚   â”œâ”€â”€ synthetic/           # Synthetic SDK
â”‚   â”œâ”€â”€ vibe/                # Vibe SDK
â”‚   â””â”€â”€ nanogpt/             # NanoGPT SDK
â”‚
â”œâ”€â”€ examples/                # Working examples
â”‚   â”œâ”€â”€ basic/              # Simple usage
â”‚   â”œâ”€â”€ multi-provider/     # Provider comparison
â”‚   â””â”€â”€ unified/            # Unified SDK usage
â”‚
â”œâ”€â”€ .claude/                # Project-specific Claude config
â”‚   â”œâ”€â”€ hooks/              # Protection hooks
â”‚   â”œâ”€â”€ skills/             # On-demand skills
â”‚   â”œâ”€â”€ memory/             # Session memory
â”‚   â””â”€â”€ optimizations/      # Token optimization
â”‚
â”œâ”€â”€ Makefile                # Build automation
â”œâ”€â”€ test-all-sdks.sh       # Test orchestration
â””â”€â”€ CLAUDE.md              # This file
```

## Key Patterns
- Each SDK is independent with own go.mod
- Zero external dependencies (pure stdlib)
- Consistent APIs across all providers
- Production-ready error handling
- Context threading for cancellation

## Hooks & Protections
Protection hooks in `.claude/hooks/`:
- `bash-protection.cjs` - Blocks destructive commands
- `antipattern-detector.cjs` - Catches stub implementations
- `suppression-abuse-detector.cjs` - Prevents hiding issues

## Skills (On-Demand)
Load from `.claude/skills/` when needed:
- `verification-before-completion/` - Completion protocol
- `systematic-debugging/` - Four-phase debugging

## Token Optimization
Directives in `.claude/optimizations/`:
- `haiku-explore.md` - Model selection guidelines
- `targeted-reads.md` - Surgical file reads
- `batched-edits.md` - Change batching strategy

## Memory
- Session diaries: `.claude/memory/diary/`
- Reflections: `.claude/memory/REFLECTIONS.md`
- claude-mem MCP server provides persistent cross-session memory

## MCP Servers

### Integrated MCP Servers
ModelScan supports Model Context Protocol (MCP) servers for extended functionality:

**Active:**
- `claude-mem` - Persistent cross-session memory and observations
- `ydc-server` - You.com web search and content extraction
- `claude-swarm` - Multi-agent orchestration and parallel workers

**Recommended:**
- **Context-Engine** 
... (truncated)


## Implementation Steps (REQUIRED)

### Phase 1: Get Your Bearings (ALWAYS START HERE)
1. Run 'pwd' to see your working directory
2. Read git logs: 'git log --oneline -20' to see recent work
3. Read claude-progress.txt (if it exists) to understand what was recently done
4. Read the feature list to understand overall progress
5. If init.sh exists, read it to understand how to run/test the project

### Phase 2: Verify Environment Health
1. Run basic tests or start development server (if applicable)
2. Verify the codebase is in a working state
3. If broken, fix critical bugs BEFORE implementing your feature
4. Document any fixes in your .done file

### Phase 3: Implement Your Feature
1. Read the relevant source files to understand current implementation
2. Make the necessary code changes using Edit or Write tools
3. Test your changes thoroughly:
   - Run automated tests (unit, integration)
   - For web features: Use Chrome DevTools MCP to test in browser as a user would
   - Verify syntax, check behavior end-to-end
   - Take screenshots of working features if applicable
4. ONLY AFTER code changes are complete AND tested: Create .claude/orchestrator/workers/feature-1.done

   The .done file MUST contain:
   - List of files you modified (with line numbers if applicable)
   - Summary of changes made to each file
   - Any tests you ran and their results
   - If BLOCKED: explain what blocked you and what you tried

### Phase 4: Leave Environment Clean
1. Do NOT commit - the orchestrator handles commits
2. Ensure all tests pass
3. Ensure code is in a working state for the next worker

## Critical Requirements
- You MUST make actual code changes - reading files is not enough
- You MUST use Edit/Write tools to modify source code
- Do NOT create the .done file until you have modified code
- If you encounter a blocker, document it in .done and explain what prevented completion
- Keep changes minimal and focused on this single feature
- NEVER commit, stage, or git add ANY of these files:
  - .claude/ (entire directory - orchestrator state, logs, prompts, worker files)
  - claude-progress.txt
  - init.sh
  - *.prompt, *.log, *.done, *.status files in .claude/


## Additional Context
# Provider Implementation: ElevenLabs TTS

## Mission
Implement ElevenLabs text-to-speech provider with **90%+ test coverage** and full validation compliance.

## Non-Negotiable Requirements

**You CANNOT mark complete unless validation passes:**
```bash
bash /home/agent/modelscan/scripts/validate-provider.sh elevenlabs 90
```

**If ANY gate fails:**
- âŒ DO NOT mark complete
- âœ… FIX the issue
- âœ… Re-run validation
- âœ… Repeat until ALL gates pass

## 7 Validation Gates
1. Build: `go build ./providers/elevenlabs.go` succeeds
2. Tests: All unit tests pass
3. Coverage: EXACTLY 90.0%+ (not 89.9%)
4. Race: `go test -race` clean
5. Vet: `go vet` clean
6. Fmt: `gofmt` clean
7. Interface: All Provider methods implemented

## Mark Complete (ONLY via validation)
```bash
bash /home/agent/modelscan/scripts/swarm-mark-complete.sh feature-1 elevenlabs
```

**No manual override. No exceptions.**

## Implementation Spec

### Files to Create
1. `providers/elevenlabs.go` - Provider implementation
2. `providers/elevenlabs_test.go` - Unit tests (90%+ coverage)

### Provider Interface
```go
type Provider interface {
    ValidateEndpoints(ctx context.Context, verbose bool) error
    ListModels(ctx context.Context, verbose bool) ([]Model, error)
    GetCapabilities() ProviderCapabilities
    GetEndpoints() []Endpoint
    TestModel(ctx context.Context, modelID string, verbose bool) error
}
```

### Use Internal HTTP Client
```go
import "github.com/jeffersonwarrior/modelscan/internal/http"

client := http.NewClient(http.Config{
    BaseURL: "https://api.elevenlabs.io/v1",
    APIKey:  apiKey,
    Timeout: 30 * time.Second,
    Retry: http.RetryConfig{
        MaxAttempts: 3,
        BaseDelay:   1 * time.Second,
    },
})
```

### ElevenLabs API Details
- Base URL: https://api.elevenlabs.io/v1
- Auth: `xi-api-key` header
- Endpoints:
  - GET /voices - List voices (models)
  - POST /text-to-speech/{voice_id} - Generate audio
  - GET /models - List TTS models
  - GET /user/subscription - Get quota info

### Test Requirements (Minimum 15 tests)
- NewElevenLabsProvider (valid key, empty key)
- ListModels (success, error, timeout, cancellation)
- ValidateEndpoints (all working, some failed, network error)
- TestModel (success, invalid model, rate limit, auth failure)
- GetCapabilities (correct values)
- Rate limiting tests
- Cost tracking tests
- TextToSpeech helper method tests

## API Documentation
- Official: https://elevenlabs.io/docs/api-reference/text-to-speech
- Models: https://elevenlabs.io/docs/voices/overview
- Pricing: Character-based billing ($180 per 1M chars Creator tier)

## Git Workflow
```bash
# Work on main branch (sequential execution)
git add providers/elevenlabs.go providers/elevenlabs_test.go
git commit -m "feat(elevenlabs): TTS provider with 90%+ coverage

âœ… All validation gates passing
âœ… Coverage: XX.X%
âœ… Tests: XX passing
âœ… Interface fully implemented

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

## Philosophy
> "90% means 90%. Not 89.9%. Exact compliance."

The validation script is the source of truth. No human judgment, no negotiation.

**Ready? Implement, test, validate, complete.**

Begin implementing the feature now.